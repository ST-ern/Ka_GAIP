import os
import numpy as np
import torch
from torch import optim

from model.utils import GAN_loss, ImagePool
from model.base_model import BaseModel
from model.intergrated import IntegratedModelGIP
from option_parser import try_mkdir


class GAN_model_GIP(BaseModel):
    def __init__(self, args, dataset, std_paths=None, log_path=None):
        super(GAN_model_GIP, self).__init__(args, log_path=log_path)
        self.character_names = ['Smpl']
        self.dataset = dataset
        self.args = args
        self.std_path = std_paths
        self.epochCount = 0

        model_GIP = IntegratedModelGIP(args)
        self.models = model_GIP
        self.D_para = model_GIP.D_parameters()
        self.G_para = model_GIP.G_parameters()

        self.criterion_rec = torch.nn.MSELoss()
        # self.optimizerD = optim.RMSprop(self.D_para, args.learning_rate / 20.0) # /10.0
        # self.optimizerG = optim.RMSprop(self.G_para, args.learning_rate / 10.0) # 不处理
        self.optimizerD = optim.Adam(self.D_para, args.learning_rate / 10.0, betas=(0.9, 0.999))
        self.optimizerG = optim.Adam(self.G_para, args.learning_rate, betas=(0.9, 0.999))
        self.optimizers = [self.optimizerD, self.optimizerG]
        self.criterion_gan = GAN_loss(args.gan_mode).to(self.device)
        # self.criterion_cycle = torch.nn.L1Loss()
        # self.criterion_ee = Criterion_EE(args, torch.nn.MSELoss())
        self.fake_pools = ImagePool(args.pool_size)

    def set_input(self, motions):
        self.motions_input = motions    # 包括了[2, motion, character], 其中、motion:tensor[1,C,t_w], character:一个int

    def discriminator_requires_grad_(self, requires_grad):
        # for model in self.models:
        model = self.models
        for para in model.discriminator.parameters():
            para.requires_grad = requires_grad

    def generator_requires_grad_(self, requires_grad):
        # for model in self.models:
        model = self.models
        for para in model.auto_encoder.parameters():
            para.requires_grad = requires_grad

    def forward(self):
        imus, poseGT = self.motions_input  # [n,t,C(24*3+72)]
        n,t,d = imus.size()
        # ============ tp的统一训练 ========================================================
        # pose6d = self.models.auto_encoder.calSMPLpose(imus)   # 原本的TP
        
        # ============ AGGRU额外训练，由tp计算的jointPos来输入AGGRU计算smpl姿势 ================
        full_joint_pos_ref = self.models.auto_encoder.calFullJointPos(imus) #[n,t,24*3]
        full_joint_pos = full_joint_pos_ref.detach()
        # full_joint_pos = joint.view(n,t,-1).detach()
        full_pos = torch.concat((full_joint_pos.new_zeros(n, t, 3), full_joint_pos), dim=-1)
        acc = imus[:,:,:18].view(n,t,6,3)
        ori = imus[:,:,18:].view(n,t,6,9)
        full_acc = acc.new_zeros(n, t, 16, 3)
        full_ori = acc.new_zeros(n, t, 16, 9)
        imu_pos = [14,15,4,5,11,0]        
        full_acc[:,:,imu_pos] = acc  # 左手右手，左腿右腿，根节点
        full_ori[:,:,imu_pos] = ori  # 左手右手，左腿右腿，根节点
        full_acc = full_acc.view(n,t,-1)
        full_ori = full_ori.view(n,t,-1)
        input = torch.concat((full_pos, full_acc, full_ori), dim=-1)
        pose6d = self.models.pose_encoder(input)
        
        self.res_pose = pose6d
        self.gt_pose = poseGT   # [n,t,90]
        
        self.epochCount += 1


    def backward_D_basic(self, netD, real, fake):
        """Calculate GAN loss for the discriminator
        GAN网络中判别器D的反向传播！
        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator
        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """
        # Real
        pred_real = netD(real)
        loss_D_real = self.criterion_gan(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())     # 通过detach断开了反向传播链，所以前面生成器的参数不会更新！
        loss_D_fake = self.criterion_gan(pred_fake, False)
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        return loss_D

    def backward_D(self):
        self.loss_Ds = []
        self.loss_D = 0
        """
        A->A
        """
        # for i in range(self.n_topology):
        fake = self.fake_pools.query(self.res_pose)
        self.loss_Ds.append(self.backward_D_basic(self.models.discriminator, self.gt_pose.detach(), fake))
        self.loss_D += self.loss_Ds[-1]
        self.loss_recoder.add_scalar('D_loss', self.loss_Ds[-1])

    def backward_G(self, backward=True):
        self.loss_G = 0

        r'''生成器计算损失 & 反向传播'''
        #rec_loss and gan loss

        # 重建损失 L_rec
        # for i in range(self.n_topology):
        rec_loss = self.criterion_rec(self.gt_pose, self.res_pose)
        self.loss_recoder.add_scalar('rec_loss_r6d', rec_loss)
        self.rec_loss = rec_loss

        # for src in range(self.n_topology):
        #     for dst in range(self.n_topology):
        # # 潜在一致性损失L_ltc
        # cycle_loss = self.criterion_cycle(self.latents_ref, self.latents)
        # self.loss_recoder.add_scalar('cycle_loss', cycle_loss)
        # self.cycle_loss += cycle_loss
        # # 末端执行器损失L_ee
        # ee_loss = self.criterion_ee(self.ee_ref, self.res_ee)
        # self.loss_recoder.add_scalar('ee_loss', ee_loss)
        # self.ee_loss += ee_loss
        
        # GAN损失，应该指的是输出的判别结果与标注（只训练生成器时标注为True）之间的损失差 -> 就是L_adv
        if self.args.gan_mode != 'none':
            loss_G = self.criterion_gan(self.models.discriminator(self.res_pose), True)
        else:
            loss_G = torch.tensor(0)
        self.loss_recoder.add_scalar('G_loss', loss_G)
        self.loss_G += loss_G

        self.loss_G_total = self.rec_loss * self.args.lambda_rec + \
                            self.loss_G * 1
                            # self.ee_loss * self.args.lambda_ee / 2 +\
                            # self.cycle_loss * self.args.lambda_cycle / 2
        if backward:
            self.loss_G_total.backward()        # 反向传播

    def optimize_parameters(self):
        r'''正向传播+反向传播的过程'''
        self.forward()

        # update Gs
        # 先更新生成器的参数
        self.discriminator_requires_grad_(False)    # 停止判别器的参数更新
        self.optimizerG.zero_grad()
        self.backward_G()                           # 计算论文中提到的4项损失，进行反向推导
        self.optimizerG.step()

        # update Ds
        # 再更新判别器的参数
        if self.args.gan_mode != 'none':# and self.epochCount % 2 == 0:
            self.discriminator_requires_grad_(True) # 开始判别器参数更新（并没有停止生成器参数的更新？）
            self.optimizerD.zero_grad()
            self.backward_D()
            self.optimizerD.step()
        else:
            self.loss_D = torch.tensor(0)   # 不使用GAN，则不训练、也不更新判别器的参数

    def verbose(self):
        res = {'rec_loss': self.rec_loss.item(),
            #    'cycle_loss': self.cycle_loss.item(),
            #    'ee_loss': self.ee_loss.item(),
               'D_loss_gan': self.loss_D.item(),
               'G_loss_gan': self.loss_G.item(),
               }
        return sorted(res.items(), key=lambda x: x[0])

    def save(self, suffix=None):
        if suffix:
            self.model_save_dir = str(suffix)
        # for i, model in enumerate(self.models):
        self.models.save(os.path.join(self.model_save_dir, 'topology'), self.epoch_cnt)

        for i, optimizer in enumerate(self.optimizers):
            file_name = os.path.join(self.model_save_dir, 'optimizers/{}/{}.pt'.format(self.epoch_cnt, i))
            try_mkdir(os.path.split(file_name)[0])
            torch.save(optimizer.state_dict(), file_name)

    def load(self, epoch=None, suffix=None, loadOpt=True):
        # for i, model in enumerate(self.models):
        if suffix:
            self.model_save_dir = str(suffix)
            
        self.models.load(os.path.join(self.model_save_dir, 'topology'), epoch)

        # 换了优化器，所以不加载原本的
        if self.is_train and loadOpt:
            for i, optimizer in enumerate(self.optimizers):
                file_name = os.path.join(self.model_save_dir, 'optimizers/{}/{}.pt'.format(epoch, i))
                optimizer.load_state_dict(torch.load(file_name))
        self.epoch_cnt = epoch


    def SMPLtest(self, motions_input):
        with torch.no_grad():
            imu, motion, root = motions_input  # [n,C(4v-4+3),t_w(64)],  一个int对应character的序号
            res = self.testForward(imu)
            
            rec_loss = self.testLoss(motion, res)
            
            smplPoseGT = self.models.auto_encoder._reduced_glb_6d_to_full_local_mat(root, motion)
            smplPoseRes = self.models.auto_encoder._reduced_glb_6d_to_full_local_mat(root, res)
            return rec_loss, smplPoseGT, smplPoseRes
        
    def testForward(self, input, joint=None):
        # ============ tp的统一训练 ========================================================
        self.models.auto_encoder.eval()
        pose6d =  self.models.auto_encoder.calSMPLpose(input)
        
        # ============ AGGRU额外训练，由tp计算的jointPos来输入AGGRU计算smpl姿势 ================
        # n,t,d = input.size()
        # full_joint_pos_ref = self.models.auto_encoder.calFullJointPos(input) #[n,t,24*3]
        # full_joint_pos = full_joint_pos_ref.detach()
        # # full_joint_pos = joint.view(n,t,-1).detach()
        # full_pos = torch.concat((full_joint_pos.new_zeros(n, t, 3), full_joint_pos), dim=-1)
        # acc = input[:,:,:18].view(n,t,6,3)
        # ori = input[:,:,18:].view(n,t,6,9)
        # full_acc = acc.new_zeros(n, t, 16, 3)
        # full_ori = acc.new_zeros(n, t, 16, 9)
        # imu_pos = [14,15,4,5,11,0]        
        # full_acc[:,:,imu_pos] = acc  # 左手右手，左腿右腿，根节点
        # full_ori[:,:,imu_pos] = ori  # 左手右手，左腿右腿，根节点
        # full_acc = full_acc.view(n,t,-1)
        # full_ori = full_ori.view(n,t,-1)
        # input_data = torch.concat((full_pos, full_acc, full_ori), dim=-1)
        # pose6d = self.models.pose_encoder(input_data)
        
        return pose6d

    
    def testLoss(self, poseGT, poseRes):
        rec_loss = self.criterion_rec(poseGT, poseRes)
        if self.args.is_train:
            self.loss_recoder.add_scalar('rec_loss_r6d_val', rec_loss)
        return rec_loss
    
    def compute_test_result(self):
        print("tmp useless")
     